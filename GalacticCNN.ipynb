{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44f6363a-e1be-48ca-84b6-5ad9ed8918d5",
   "metadata": {},
   "source": [
    "# Galactic Classification CNN\n",
    "\n",
    "## Purpose:\n",
    "\n",
    "The purpose of this was for me to gain more exposure to CNNs in a context I find rather interesting. I don't have much experience with NNs in general, so I thought building one (admittedly a rather basic one) would be a good start! This model will also serve as a base-line for when I get access to more computing power and memory, or a general conceptual guide when I need to come back for a quick reference. \n",
    "\n",
    "## Section Breakdown:\n",
    "\n",
    "I have a description of each cell above it so please look there if you want a summary/explanation, but the general index is something like this (and note that each nth cell refers to the coded/non-markdown sections):\n",
    "\n",
    "1. First Cell: general set-up for my local enviornment and importing the first needed packages\n",
    "2. Second Cell: Matching the images to file names as it wasn't set up automatically\n",
    "3. Third Cell: Image configuration and adding images/labels to their respective lists\n",
    "4. Fourth Cell: Converting lists to numpy arrays, splitting into test/train sets\n",
    "5. Fifth Cell: Model Architecture\n",
    "6. Sixth Cell: Hyper-parameter tuning\n",
    "7. Seventh Cell: Model training and saving\n",
    "\n",
    "## Limitations:\n",
    "\n",
    "The biggest hurdle for me by far was the fact that I am running on a local machine with 8GB of RAM. I had to choose between sacrificing some number of layers, reducing the training/validation set sizes, using a model that isn't so hard on memory, and so on and so forth to be able to run this locally. \n",
    "\n",
    "## Discussion:\n",
    "\n",
    "Overall I'm happy with the model. The accuracy and loss values were better than I was expecting when I first started building this 'toy model', and I'm sure if I put more elbow-work into this model I could improve both of the aforementioned metrics (see 'Future Work for a few ideas)\n",
    "\n",
    "## Future Work:\n",
    "\n",
    "* Run it with the full dataset \n",
    "* Change parameters (optimizer, train/test set percentages, etc)\n",
    "* Add more layers \n",
    "* Incorporate rotations and flips\n",
    "\n",
    "## Final notes: \n",
    "\n",
    "The explanations on this are more conceptually than mathematically based. For more mathematical based explanations, I can direct you to Google's ML explanation, Andrew NG's ML courses, Codecademy's ML courses, or most other pages out there centered around ML/CV/NNs. I originally built and ran this in a .py file that will also be in my github, however I figured it would be better for the reasons described in the 'Purpose' section to put it in a notebook, split different sections into different cells, and use Markdown cells to annotate my notes/thoughts. However, for those who prefer to use VSCode/PyCharm/XYZ IDE - you are welcome to use the .py file rather than this. They should be the same as this was just copied and pasted from there (with the addition of the Markdown cells of course). Thanks for reading :)\n",
    "\n",
    "## Data Source: \n",
    "\n",
    "Thanks to the wonderful people at Galaxy Zoo for this data! The images and CSV files were pulled from here: https://data.galaxyzoo.org/\n",
    "\n",
    "_____________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959b9420-75d4-424b-991f-45906c8d7b21",
   "metadata": {},
   "source": [
    "### Cell One: \n",
    "\n",
    "This is the general set-up for my local environment, some of the os.environ lines would not be needed if run on other machines. Importing the first  and commonly used packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7156cccd-7991-43cf-a9ba-90439fd4af66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "os.environ['QT_QPA_PLATFORM'] = 'offscreen'\n",
    "os.environ['QT_LOGGING_RULES'] = 'qt.qpa.*=false'\n",
    "uid = 1000\n",
    "os.environ['XDG_RUNTIME_DIR'] = f'/run/user/{os.getuid()}'\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from PyQt5.QtCore import QLibraryInfo\n",
    "\n",
    "\n",
    "\n",
    "os.environ['QT_PLUGIN_PATH'] = os.path.join(QLibraryInfo.location(QLibraryInfo.PluginsPath))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ca2db0-67b9-4cda-b7b2-93d3457e16dd",
   "metadata": {},
   "source": [
    "### Cell Two: \n",
    "\n",
    "This matches the file name with the corresponding row in the corresponding CSV file. When initially downloaded, the way the images were unpacked left the 250,000 image folder in a completely randomized state, so this goes through and matches the correct file to the correct row, and then adds them to a list. It stops at 5000 as that is the size that is big enough to train the data on, but not so large that it causes issues when running (refer to the 'Limitations' section in the very beginning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e9828ef-911e-4949-b035-74ebe66bd316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get a list of all image file names in the folder\n",
    "galaxy_df = pd.read_csv(\"/home/cyrus/Documents/galaxy_df_final.csv\")\n",
    "\n",
    "# Set the path to the directory containing the images\n",
    "image_directory = \"/home/cyrus/Downloads/images_gz2/images\"\n",
    "\n",
    "# Initialize empty lists to store the matched image file names and their corresponding 'gz2class' values\n",
    "matched_images = []\n",
    "matched_labels = []\n",
    "\n",
    "# Iterate through the rows in the table\n",
    "for index, row in galaxy_df.iterrows():\n",
    "\n",
    "    if len(matched_images) >= 5000:\n",
    "        break\n",
    "    \n",
    "    # Extract the 'id' value\n",
    "    image_id = row[\"asset_id\"]\n",
    "\n",
    "    # Create the corresponding image file name\n",
    "    image_file = f\"{image_id}.jpg\"\n",
    "\n",
    "    # Check if the image file exists in the directory\n",
    "    if os.path.isfile(os.path.join(image_directory, image_file)):\n",
    "        # Add the image file name to the matched_images list\n",
    "        matched_images.append(image_file)\n",
    "        # Add the corresponding 'gz2class' value to the matched_labels list\n",
    "        matched_labels.append(row[\"gz2class\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5434a1-5665-4082-a02d-9c32e350b947",
   "metadata": {},
   "source": [
    "### Cell Three: \n",
    "\n",
    "This sets up the resizing and reformatting of the images. Resizing from a 4XXp to 128p sized image helps save RAM, and OpenCV by default uses a BGR color format, so I conert it to the standard RGB that we all know an love! It also loops through in batches to help save on RAM (again, one of the tedious limitations of my local machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc2c2f24-5daa-4902-a1f6-966ff7cd306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the desired dimensions for resizing\n",
    "resize_dim = (128, 128)\n",
    "\n",
    "# Set the batch size\n",
    "batch_size = 10\n",
    "\n",
    "# Initialize an empty list to store the batches of images and labels\n",
    "image_batches = []\n",
    "label_batches = []\n",
    "\n",
    "# Loop through the image files and labels in batches\n",
    "images = []\n",
    "for i in range(0, len(matched_images), batch_size):\n",
    "    batch_images = matched_images[i:i + batch_size]\n",
    "    batch_labels = matched_labels[i:i + batch_size]\n",
    "    image_arrays = []\n",
    "\n",
    "    for image_file in batch_images:\n",
    "        image_path = os.path.join(image_directory, image_file)\n",
    "\n",
    "        # Read the image from file\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        if image is not None:\n",
    "            # Convert the image from BGR to RGB format\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Resize the image\n",
    "            resized_image = cv2.resize(image, resize_dim)\n",
    "\n",
    "            # Append the resized image to the list of image arrays\n",
    "            images.append(resized_image)\n",
    "\n",
    "    # Add the batch of image arrays and labels to their respective lists\n",
    "    # image_batches.append(image_arrays)\n",
    "    label_batches.extend(batch_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d840b1b9-eccd-49f1-96ad-b422a165c2d6",
   "metadata": {},
   "source": [
    "### Cell Four:\n",
    "\n",
    "This converts the previous lists to numpy arrays, it then normalizes the pixel values, encodes the labels, and splits the data into training and test sets.\n",
    "\n",
    "Encoding is important because it converts the data from  a type the model can't read to a type that it can. It changes the labels from a categorical data type to numerical. \n",
    "\n",
    "Test train splits are usually recommended to be about 70% train and 30% test, 70% train, 15% test, and 15% validation, or 70% train, 20% test, and 10% validation - however, I chose to do 80% for training and 20% for testing. There are many different way to set it up with varying philosophies and degrees of success, but for this 'toy model' I thought that the layers would be more important. An exploration into different test-train split values could yield higher accuracies in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb35caa8-8f9f-4250-9d08-348d835e1c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 16:56:56.340590: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "X = np.array(images)\n",
    "y = np.array(label_batches)\n",
    "\n",
    "\n",
    "# Normalize pixel values\n",
    "X = X.astype('float32')\n",
    "X /= 255.0\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode labels\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(y)\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Assuming your data is in the following format\n",
    "# X = np.array(images_array)  # Images as numpy arrays\n",
    "# y = np.array(matched_labels)       # Labels as integer values\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size = 0.2, random_state = 42, stratify = y_encoded)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a99a37-5a84-47cf-9dc9-fa03ca36795b",
   "metadata": {},
   "source": [
    "### Cell Five:\n",
    "\n",
    "This is the model architecture, where the model is a Sequential model with 2x Convolution layers, 2x Pooling layers, a Flattening layer, and 2x Dense layers. Here is a breakdown of the specifics of each term:\n",
    "\n",
    "* Sequential model: a model with multiple layers where each layer takes a tensor as an input, and has a tensor as an output. It is a simple model so it is computationally inexpensive \n",
    "* Convolution layers: takes a 2D input, applies a specificed number of filters, and then outputs a transformed version of the inputs. The filters move the same way English is read (top to bottom, left to right), and applies the transformation one pixel at a time. A few parameters:\n",
    "   * filters: specifies what filters and how many will be used\n",
    "   * kernel size: the 2D size of the filter (height x width)\n",
    "   * strides: The length of the \"step\" in pixels, so a stride of 1 will mean that each time a transformation is finished, it will move one pixel over\n",
    "   * padding: This deals with the border or edge of the input. 'Valid' means that the filter doesn't go past the boundary, and 'same' means that the input has the same size as the original input\n",
    "   * activation: This specifies what funciton is applied to the output, and I used 'ReLu' in this particular model\n",
    "   \n",
    "* Pooling Layers: in the specific case of MaxPooling, it finds the max value of each part of a feature map to find the important parts like edges. In general, a pooling layer is applied after a convolution layer, and is a downsampling technique that decreases computational cost by reducing the 2D size of the filter (heigh x widght), and also provides translational variance to help reduce overfitting. A few parameters:\n",
    "\n",
    "   * pool_size: this is the 2D size of the pooling frame (in height x width dimensions)\n",
    "   * strides: The length of the \"step\" in pixels, so a stride of 1 will mean that each time a transformation is finished, it will move one pixel over\n",
    "   * padding: This deals with the border or edge of the input. 'Valid' means that the filter doesn't go past the boundary, and 'same' means that the input has the same size as the original input\n",
    "\n",
    "* Flattening Layers: usually after the Convolution and Pooling layers comes the Flattening layer(s). This layer takes a multidimensional input (in this case a 2D input of height x weight), and turns it into a 1D tensor. If the multidimensional input is 3D shape of 5x5x2, the output of the tensor would be 50.\n",
    "* Dense Layers: a Dense layer is a full connected layer, meaning that each \"neuron\" is connected to each neuron in the previous layer. It takes the features from the other layers and predicts them to make predictions. Each neuron takes each input, adds weights and biases to each, and applies to specified activation function\n",
    "* 'ReLu': the Rectified Linear Unit is an activation function that returns 0 the input is a negative value, and returns x for any other value. It also introduces non-linearity to the model, and allows it to handle \"real world data\" a little better. ReLu is popular because it is avoids the gradient problem (where gradients that are extremely small make it hard to account for) by having the positive inputs always equal 1.\n",
    "* 'Dense units': Dense units are the individual 'neurons' or 'nodes' in a dense layer. Dense(64) would refer to a dense layer of 64 neurons\n",
    "* 'Softmax': An activation function that normalizes the values of the inputs between zero and one, and assigns a class based on the probability of it falling within the determined range. Often used in multi-class classification problems where each image can belong to a number of categories, and also generally the last layer in a CNN for image classifiation. \n",
    "* 'seed': A seed is a number that we pass to the random number generator to initialize it. If we use the same seed every time, then the random number generator will produce the same sequence of random numbers every time - this allows for reproductibility when running the model. Without it, even when using the same training data and initial conditions (i.e. epochs), the initial weights would be different every time it was ran. \n",
    "* 'Adam optimizer': Adaptive Moment Estimation, Adam accounts for bias correction by incorporating first and second order moments of the gradient. This means it accounts for just the previous step, and it happens with each update. It calculates an exponential moving average of the gradient and the squared gradient, and the parameters beta1 and beta2 control the decay rates of these moving averages. Adam requires less tuning than other optimizers, and handles noise functions well, but also has the drawback of being more computationally expensive. \n",
    "\n",
    "The filters I wasn't sure what to name, so I followed the \"standard notation\" of dataframes (i.e. the first dataframe would be df or df1, the second would be df2, etc etc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6dad2f2-40be-4fd7-9f3d-fdacccd68ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from keras_tuner import HyperModel\n",
    "\n",
    "class GalacticHyperModel(HyperModel):\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = models.Sequential()\n",
    "        \n",
    "        # Convolutional and pooling layers with tunable hyperparameters\n",
    "        model.add(layers.Conv2D(filters = hp.Int('filters_1', 16, 64, step = 16),\n",
    "                                kernel_size = (3, 3),\n",
    "                                activation = 'relu',\n",
    "                                input_shape = self.input_shape))\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "        \n",
    "        model.add(layers.Conv2D(filters = hp.Int('filters_2', 32, 128, step = 32),\n",
    "                                kernel_size = (3, 3),\n",
    "                                activation = 'relu'))\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "        \n",
    "        model.add(layers.Conv2D(filters = hp.Int('filters_3', 64, 256, step = 64),\n",
    "                                kernel_size = (3, 3),\n",
    "                                activation = 'relu'))\n",
    "\n",
    "        # Fully connected layers\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(units = hp.Int('dense_units', 32, 256, step = 32),\n",
    "                               activation = 'relu'))\n",
    "        model.add(layers.Dense(self.num_classes, activation = 'softmax'))\n",
    "\n",
    "        # Compile the model with tunable learning rate\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate = hp.Float('learning_rate', 1e-4, 1e-2, sampling = 'log'))\n",
    "        # optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "        model.compile(optimizer = optimizer, loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "        return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b299cc-8763-4ba2-a1b3-83589bf0e684",
   "metadata": {},
   "source": [
    "### Cell Six:\n",
    "\n",
    "Here is the actual implementation of the model. The hypermodel is instantiated, the tuning parameters are set up, and then the tuning process iterates over 50 different epochs (or sets) and finds the best combination of parameters. Early stopping is also included, so if the model isn't improving much in terms of validation accuracy, the model moves onto the next epoch. It basically decides that the time that could be spent training on the current epoch would be better spent on others, as there are limited and diminishing returns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95d03d5d-31f3-4acc-aab1-98e5e5be5212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from ./galaxy_cnn_tuning/tuner0.json\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 16:56:59.762069: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras_tuner import RandomSearch\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "\n",
    "input_shape = (128, 128, 3)\n",
    "num_classes = 3\n",
    "\n",
    "# Instantiate the HyperModel\n",
    "galactic_hypermodel = GalacticHyperModel(input_shape, num_classes)\n",
    "\n",
    "# Set up the RandomSearch tuner\n",
    "tuner = RandomSearch(galactic_hypermodel,\n",
    "                     objective = 'val_accuracy',\n",
    "                     max_trials = 30,  # Number of different hyperparameter combinations to try\n",
    "                     seed = 42,\n",
    "                     project_name = 'galaxy_cnn_tuning')\n",
    "\n",
    "# Search for the best hyperparameters\n",
    "tuner.search(X_train, y_train,\n",
    "             epochs = 50,\n",
    "             validation_data = (X_val, y_val),\n",
    "             callbacks = [tf.keras.callbacks.EarlyStopping(patience=5)])\n",
    "\n",
    "# Get the best model found by the tuner\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "galaxy_cnn = tuner.hypermodel.build(best_hp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817302c7-2e34-40fd-a1eb-91517957aa29",
   "metadata": {},
   "source": [
    "### Cell Seven:\n",
    "\n",
    "This is where the model is actually ran. The results are then saved into a pickle file and then the results are analyzed/plotted in a separate environment. I've seen other people do everything in one notebook, however I was having issues plotting in this notebook for some reason so I decided to split them up. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85c79479-1f72-4711-840c-ae21f1a57b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 16:57:00.351570: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 786432000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 433s 3s/step - loss: 0.8334 - accuracy: 0.6090 - val_loss: 0.7164 - val_accuracy: 0.7190\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 435s 3s/step - loss: 0.7093 - accuracy: 0.7060 - val_loss: 0.6571 - val_accuracy: 0.7510\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 432s 3s/step - loss: 0.6484 - accuracy: 0.7293 - val_loss: 0.6316 - val_accuracy: 0.7670\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 433s 3s/step - loss: 0.6007 - accuracy: 0.7525 - val_loss: 0.6197 - val_accuracy: 0.7610\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 432s 3s/step - loss: 0.5468 - accuracy: 0.7778 - val_loss: 0.6245 - val_accuracy: 0.7470\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 433s 3s/step - loss: 0.5008 - accuracy: 0.7972 - val_loss: 0.6278 - val_accuracy: 0.7500\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 433s 3s/step - loss: 0.4564 - accuracy: 0.8163 - val_loss: 0.6486 - val_accuracy: 0.7380\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 436s 3s/step - loss: 0.4074 - accuracy: 0.8390 - val_loss: 0.6730 - val_accuracy: 0.7450\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 434s 3s/step - loss: 0.3592 - accuracy: 0.8605 - val_loss: 0.7439 - val_accuracy: 0.7310\n",
      "Finished with the model!\n"
     ]
    }
   ],
   "source": [
    "# Set hyperparameters\n",
    "import tensorflow as tf\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Compile the model with the chosen learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "galaxy_cnn.compile(optimizer = optimizer, loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Set up early stopping to avoid overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = galaxy_cnn.fit(X_train, y_train,\n",
    "                         batch_size = batch_size,\n",
    "                         epochs = epochs,\n",
    "                         validation_data = (X_val, y_val),\n",
    "                         callbacks = [early_stopping])\n",
    "\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"history.pickle\", \"wb\") as f:\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "\n",
    "print('Finished with the model!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e290902-f941-4b9e-8ab1-11838536a2bb",
   "metadata": {},
   "source": [
    "#### Finished with the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bbbdf3-d519-42fc-a11b-0bf3b987876d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
